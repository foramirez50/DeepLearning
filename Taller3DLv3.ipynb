{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foramirez50/DeepLearning/blob/main/Taller3DLv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://docs.google.com/uc?export=download&id=1NUy1Q-abpoV9XYK9qT9t8Mdhj3ZVlveO)"
      ],
      "metadata": {
        "id": "znKNaJnn_ipc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCYoS7vchGU"
      },
      "source": [
        "\n",
        "# **Taller 3**\n",
        "\n",
        "La comunidad científica es de la comunidad más activa que existe a la hora de realizar publicaciones en grandes revistas. Los avances tecnológicos, las facilidades a la hora de encontrar información y la gran cantidad de médicos dedicados a realizar estudios y publicarlos ha aumentado a tal punto que realizar una categorización de estos textos sería ideal para poder filtrar la información y acceder a ella de una manera más rápida y precisa.\n",
        "\n",
        "Para eso, le han disponibilizado los *Abstracts* de diferentes artículos de los cuales los expertos los han clasificado en diferentes categorias, la idea es poder automatizar este proceso para que futuros artículos puedan ser clasificados automáticamente sin necesidad de disponer del tiempo de los expertos.\n",
        "\n",
        "Para resolver el modelo, es importante que construyan la mejor Red Neuronal Recurrente que puedan encontrar. El procesamiento, la tokenización, la limpieza de los textos, la definición de los Embedding y la arquitectura de la Red Neuronal, así como la utilización de Redes Preentrenadas están a su criterio. Lo importante es documentar todos los pasos anteriormente descritos.\n",
        "\n",
        "Datos: https://raw.githubusercontent.com/Camilojaravila/202210_MINE-4206_ANALISIS_CON_MACHINE_LEARNING/main/Taller%202/train.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INTEGRANTES**\n",
        "\n",
        "FABIAN ORLANDO RAMIREZ\n",
        "\n",
        "DAVID AUGUSTO VASQUEZ"
      ],
      "metadata": {
        "id": "s3xhxYEJf6NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSTALACIÓN DE LIBRERÍAS Y SUBIDA DEL DATASET"
      ],
      "metadata": {
        "id": "HBH5lbF52rkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "print('Tensorflow version:', tf.__version__)"
      ],
      "metadata": {
        "id": "c5ecxP4huCHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7b83da-c78b-4c29-da9c-4efc4ebe7e3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version: 2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Camilojaravila/202210_MINE-4206_ANALISIS_CON_MACHINE_LEARNING/main/Taller%202/train.dat\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file('/content/abstract.dat', url)"
      ],
      "metadata": {
        "id": "x-w7a9I8fluE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"/content/abstract.dat\"\n",
        "with open(file) as f:\n",
        "    mylist = [line.rstrip('\\n') for line in f]"
      ],
      "metadata": {
        "id": "6m_8cPgMVQQo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# este procedimiento se hizo para mostrar que el dataset está desbalanceado, \n",
        "df = pd.DataFrame (mylist, columns = ['class'])\n",
        "df[['labels','text']] = df['class'].str.split('\\t',1,expand=True)\n",
        "df.labels.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oO5-5mCW9hx",
        "outputId": "89f995b0-82a7-4794-e6a7-cbc3cc72778c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    4805\n",
              "1    3163\n",
              "4    3051\n",
              "3    1925\n",
              "2    1494\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRITERIO 1: PROCESAMIENTO DE TEXTO"
      ],
      "metadata": {
        "id": "BAQ45J_1THmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# se convierte el dataset de .dat a un dataset de tf con la librería TextLineDataset\n",
        "ds_tensor = tf.data.TextLineDataset(dataset)"
      ],
      "metadata": {
        "id": "VYQmmaT1QBa4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se ve el tipo y la clase de dataset que se tiene TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)\n",
        "print(\"dataset class............\", type(ds_tensor))\n",
        "print(\"dataset type.............\",ds_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzOwBau2UBBr",
        "outputId": "93216066-8962-46ec-eb6e-9c43d7f0a837"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset class............ <class 'tensorflow.python.data.ops.readers.TextLineDatasetV2'>\n",
            "dataset type............. <TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con esta función, dividimos los vectores en labels y text, pero además de esto le hacemos un cast para definir los tipos (dtypes)\n",
        "def get_split_tensor(text):\n",
        "  dataset_split = tf.strings.split(text,sep='\\t')\n",
        "  call_label = tf.cast(dataset_split[1], tf.string)\n",
        "  call_text = tf.cast(int(dataset_split[0]),tf.int64)\n",
        "  return call_label , call_text"
      ],
      "metadata": {
        "id": "7MKykHGyRVt6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se hace un mapeo para la tranformación con la función de spli y cast los tensores\n",
        "ds_tensor_splited = ds_tensor.map(get_split_tensor)"
      ],
      "metadata": {
        "id": "x5cEk3MtTFV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en esta parte se ve la transformación del .map a MapDataset\n",
        "# aquí se puede observar las 2 partes del tensor, la parte del label (int.64) y la parte del texto (string)\n",
        "print(\"dataset class............\", type(ds_tensor_splited))\n",
        "print(\"dataset type.............\",ds_tensor_splited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuh7sT7ZTko2",
        "outputId": "ffd51a54-d9bb-4cf0-8742-c71b2a9befc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset class............ <class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
            "dataset type............. <MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# damos un ejemplo con el label y el texto\n",
        "for text, label in ds_tensor_splited.take(1):\n",
        "  print(\"label: \", label.numpy())\n",
        "  print(\"Text: \", text.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vx0ZrwVRACc",
        "outputId": "7cd7fbc8-81c5-4bcf-e432-72313c35b2a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  4\n",
            "Text:  Catheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic therapy, catheterization laboratory and hospital events were assessed in consecutively treated patients with infarctions involving the left anterior descending (n = 100 patients), right (n = 100), and circumflex (n = 50) coronary arteries. The groups of patients were similar for age (left anterior descending coronary artery, 59 years; right coronary artery, 58 years; circumflex coronary artery, 62 years), patients with multivessel disease (left anterior descending coronary artery, 55%; right coronary artery, 55%; circumflex coronary artery, 64%), and patients with initial grade 0/1 antegrade flow (left anterior descending coronary artery, 79%; right coronary artery, 84%; circumflex coronary artery, 90%). Cardiogenic shock was present in eight patients with infarction of the left anterior descending coronary artery, four with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery. Major catheterization laboratory events (cardioversion, cardiopulmonary resuscitation, dopamine or intra-aortic balloon pump support for hypotension, and urgent surgery) occurred in 10 patients with infarction of the left anterior descending coronary artery, eight with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery (16 of 16 shock and six of 234 nonshock patients, p less than 0.001). There was one in-laboratory death (shock patient with infarction of the left anterior descending coronary artery). \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#se divide el dataset siguiendo lasl instruccions citadas abajo, con esto sabemos el tamaño del dataset\n",
        "#https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets\n",
        "DATA_SIZE = len(list(ds_tensor_splited))\n",
        "print(DATA_SIZE)"
      ],
      "metadata": {
        "id": "MGXu7SqDfCor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8484551-654b-4a59-8756-085f48ec385c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# se toma la función hstack para tomas la \"y\" que serían los labels en el vector\n",
        "# con esto se encontró los labels del dataset\n",
        "labels1 = np.hstack([y for x, y in ds_tensor_splited])\n",
        "CATEGORIES = np.unique(labels1)\n",
        "print(CATEGORIES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeLanH_J20iY",
        "outputId": "592a87a9-f3e2-4d47-a09f-3aa97b16c96b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# El dataset se divide en 20% de test, 16% para validación (el 20% del train) y finalmente el train con un 64%\n",
        "train_size = int(0.64 * DATA_SIZE)\n",
        "val_size = int(0.16 * DATA_SIZE)\n",
        "test_size = int(0.20 * DATA_SIZE)"
      ],
      "metadata": {
        "id": "LLRXC0Daa2HT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se dividen los datasets y \"saltan\" las partes que están ya en otros grupos\n",
        "train_ds = ds_tensor_splited.take(train_size).shuffle(1000)\n",
        "test_ds = ds_tensor_splited.skip(train_size)\n",
        "val_ds = test_ds.skip(test_size)\n",
        "test_ds = test_ds.take(test_size)"
      ],
      "metadata": {
        "id": "OfdA-Yrma64P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Se toman muestras y tamaños de los datasets de train, val y test"
      ],
      "metadata": {
        "id": "1kIpuQg5VRTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text, label in train_ds.take(1):\n",
        "  print(\"label: \", label.numpy())\n",
        "  print(\"Text: \", text.numpy().decode('utf-8'))\n",
        "  print(\"train size\", len(list(train_ds)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCJ_at5ya6zI",
        "outputId": "45c4fb6b-c133-4b29-e290-eae415d4517d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  4\n",
            "Text:  Femoral artery cannulation for monitoring in critically ill children: prospective study. Seventy-seven attempted percutaneous femoral artery cannulations were prospectively evaluated in 74 children. Artery cannulation was successfully accomplished in 73 (95%) cases and lasted for a mean of 6 days. Sixty percent of the catheters were inserted on the first attempt. Fifty-two (71%) patients weighed less than 10 kg and 55 (75%) patients were less than 12 months old. Fifty-one (70%) patients received inotropic support at the time of cannulation, and 27 (37%) eventually died from causes unrelated to catheter insertion. There was one episode each of line-associated infection and transient distal ischemia not resulting in tissue loss, and two episodes of catheter malfunction. In eight (11%) patients, signs of distal vascular insufficiency developed shortly catheter placement and resolved after catheter removal. The development of this complication correlated significantly (p less than .05) with younger age (5.5 vs. 22.3 months). We conclude that femoral artery cannulation has a high degree of success in very small, critically ill children. It should be considered an acceptable alternative to small-vessel cannulation when the latter is not technically achievable, or in the unstable patient where rapid establishment of reliable arterial access is necessary. \n",
            "train size 9240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text, label in val_ds.take(1):\n",
        "  print(\"label: \", label.numpy())\n",
        "  print(\"Text: \", text.numpy().decode('utf-8'))\n",
        "  print(\"val size\", len(list(val_ds)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HY3Fh0-xiXd",
        "outputId": "a1e410f5-765a-471f-d9f2-d799c4922519"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  1\n",
            "Text:  Interstitial chemotherapy with drug polymer implants for the treatment of recurrent gliomas. Malignant gliomas have been difficult to treat with chemotherapy. The most effective agent, BCNU (carmustine), has considerable systemic toxicity and a short half-life in serum. To obviate these problems, a method has been developed for the local sustained release of chemotherapeutic agents by their incorporation into biodegradable polymers. Implantation of the drug-impregnated polymer at the tumor site allows prolonged local exposure with minimal systemic exposure. In this Phase I-II study, 21 patients with recurrent malignant glioma were treated with BCNU released interstitially by means of a polyanhydride biodegradable polymer implant. Up to eight polymer wafers were placed in the resection cavity intraoperatively, upon completion of tumor debulking. The polymer releases the therapeutic drug for approximately 3 weeks. Three increasing concentrations of BCNU were studied; the treatment was well tolerated at all three levels. There were no adverse reactions to the BCNU wafer treatment itself. The average survival period after reoperation was 65 weeks for the first dose group, 64 weeks for the second dose group, and 32 weeks for the highest dose group. The overall mean survival time was 48 weeks from reoperation and 94 weeks from the original operation. The overall median survival times were 46 weeks postimplant and 87 weeks from initial surgery. Eighteen (86%) of 21 patients lived more than 1 year from the time of their initial diagnosis and eight (38%) of 21 patients lived more than 1 year after intracranial implantation of the polymer. Frequent hematology, blood chemistry, and urinalysis tests did not reveal any systemic effect from this interstitial chemotherapy. Since the therapy is well tolerated and safe, a placebo-controlled clinical trial has been started. The trial will measure the effect of the second treatment dose on survival of patients with recurrent malignant glioma. \n",
            "val size 2311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text, label in test_ds.take(1):\n",
        "  print(\"label: \", label.numpy())\n",
        "  print(\"Text: \", text.numpy().decode('utf-8'))\n",
        "  print(\"test size\", len(list(test_ds)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_4YGamxa6p7",
        "outputId": "e764d307-d9a4-4866-9a7a-508f635322c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  5\n",
            "Text:  Reoperations on heart valve prostheses: an analysis of operative risks and late results. To evaluate risks and complications of reoperations on heart valve prostheses, we reviewed data on 183 patients who underwent reoperation because of prosthetic valve malfunction. The incremental effect of the redo procedure on hospital mortality and morbidity was studied by comparing primary and reoperative procedures and analyzing a series of possible predisposing factors. Late survival after first and second reoperations was computed, and possible determinants of late mortality were examined. Overall operative mortality was 8.7%; emergency operation (p = 0.0001), previous thromboembolism (p = 0.05), and advanced New York Heart Association functional class (p = 0.031) were the independent determinants. In a series of 1,355 patients having primary or secondary isolated valve replacement, the redo procedure was a significant risk factor in the univariate analysis (p = 0.025) but not in the multivariate analysis except for the subset of patients having mitral valve replacement (p = 0.052). The postoperative course was quite complicated, as evidenced by the long mean stay in the intensive care unit (mean stay, 3.8 days; longer than 2 days for 26% of the survivors). Nevertheless, postoperative complications were not significantly greater after a redo procedure than after a primary operation. Actuarial survival at 7 years was 57.3% +/- 8%. A comparison with a nonhomogeneous series from our institution did not demonstrate significant differences. In the subset of 16 patients having a second reoperation, late survival was 37.8% +/- 16% at 2 years. Advanced New York Heart Association class (p = 0.0001), double prosthetic valve dysfunction (p = 0.003), and any indication other than primary tissue failure (p = 0.06) were determinants of late mortality. \n",
            "test size 2887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### se comprueba que están divididos y que sumados dan el total del dataset  9240+2311+2887 = 14438"
      ],
      "metadata": {
        "id": "zCwrXf30yWku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corremos el AUTOTUNE para optimizar el procesamiento de las variables\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "training_data = train_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "3KbvfyqG8ROc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aquí vemos que de nuevo el tensor cambio de clase a PrefetchDataset element_spec, los componentes int y string siguen igual\n",
        "print(\"dataset class............\", type(training_data))\n",
        "print(\"dataset type.............\",training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEFULpaXz2MR",
        "outputId": "6f2e6116-e1c2-437e-a2e2-68c0c6d9cbb7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset class............ <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
            "dataset type............. <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#import re\n",
        "#import string"
      ],
      "metadata": {
        "id": "Zq86Q3Op265J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se intentó hacer con la función dada por el monitor, pero eso daña el text vectorization, deja el vector con valores entre 0 y 30 y no entre 0 y 12000 como dice el vocabulario\n",
        "\n",
        "#def custom_standardization(input_data):\n",
        "#    lowercase = tf.strings.lower(input_data)\n",
        "#    stripped_html = tf.strings.regex_replace(lowercase, '', ' ')\n",
        "#    return tf.strings.regex_replace(stripped_html,\n",
        "#                                  '[%s]' % re.escape(string.punctuation), '')"
      ],
      "metadata": {
        "id": "M1tigNHB8Z7B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRITERIO 2 : ARQUITECTURA EMBEDDING"
      ],
      "metadata": {
        "id": "himTrxLpURnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.crowdwriter.com/blog/how-long-should-an-abstract-be según este articulo, el máximo len para un abstract es de 300 palabras en harvard\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=12000,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=300)"
      ],
      "metadata": {
        "id": "yLTg6pMt0Jn5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se divide un tf dataset para dejar sólo los textos\n",
        "text_ds = test_ds.map(lambda text, labels: text)\n",
        "# y se utiliza el .adapt para determinar la frecuencia de las palabras y crear el vocabulario con estas palabras\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "hCB6Gr0l-sTv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# con esto se prueba que el vocabulario esté entre los rangos (12000) \n",
        "#se observa que los abstracts que tienen menos palabras se rellenan con \"0\"\n",
        "text_batch, label_batch = next(iter(training_data))\n",
        "text = tf.expand_dims(text_batch, -1)\n",
        "print(vectorize_layer(text), label_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNPCeo_qwfwz",
        "outputId": "7685db55-b17f-4bb7-b89c-267fe5abb5d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   7    1    1  340    8    1    7 3761  270  105 8771    1   12 3165\n",
            "   841 6574  841 6295    6    7    1   13    1 1845   16  670   14    7\n",
            "  3761  270    2    3    1 3798   67   16  687 3294    6 8771    1  150\n",
            "  8771    1   96  323  631  165   23    7 3455  471    3    1    1  340\n",
            "     7  280  246    8    1    7 3761  270   10 1501    8  295    9    6\n",
            "   629   77    5    3 4545    2    7 3165  841 6574   14    3    1 1845\n",
            "     1 1845 8771    1   11   27  114    3   56   11 2421 1257   12   51\n",
            "     9   48 1597 5154  177    2 3736    2    3 3761    5   46    4 4475\n",
            "   270  146  187    8  533  372    2    3 3761  270    5    3   76 2603\n",
            "     2    3 3761  270   18   19 1071  124   99    7  685 1525 3016 3761\n",
            "   270   15    1  228    8 5048 6295    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]], shape=(1, 300), dtype=int64) tf.Tensor(1, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRITERIO 3: ENTRENAMIENTO DE LA RED NEURONAL RECURRENTE\n",
        "\n",
        "En esta parte se construyó el modelo, se utilizaron una capa de RNN tipo GRU, por su simplicidad y rapidez de entrenamiento.\n",
        "\n",
        "El Embeding es de 12000 + 1 teniendo en cuenta las palabras desconocidas\n",
        "como el embeding es de 32, se coloco el batch también de este tamaño.\n",
        "\n",
        "Se deja el optimizer Adam por defect y como lideamos con un problema de clasificación colocamos la sparseCategoricalCrossentropy como función de costo\n"
      ],
      "metadata": {
        "id": "tWQSOZCM3Fio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# en esta parte se obtenía un error en la capa de salida, pues se tiene los números de 1 a 5 en el clasificador, por esto se tuvo que sumar 1 en las neuronas de la capa de salida\n",
        "embedding_dim=32\n",
        "\n",
        "model_GRU = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  Embedding(12000 + 1, embedding_dim, name=\"Capa_Embedding\"),\n",
        "  tf.keras.layers.GRU(32, name='Capa_Celdas_GRU'),\n",
        "  Dense(32, activation='relu', name='Capa_Oculta'),\n",
        "  Dense(6,name='Capa_Salida')\n",
        "])\n",
        "\n",
        "model_GRU.compile(optimizer='adam',\n",
        "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "]\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "gT4e1zgezGJ8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se transforma a batch para poder entrenar el modelo, de lo contrario da error. \n",
        "training_data_batch = training_data.padded_batch(batch_size)\n",
        "val_data_batch = val_ds.padded_batch(batch_size)"
      ],
      "metadata": {
        "id": "uMPEcGezAvbq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se ve la tranformación en el tipo del conjunto de training y validación de prefech a batch\n",
        "print(\"dataset class............\", type(training_data_batch))\n",
        "print(\"dataset type.............\",training_data_batch)\n",
        "print(\"dataset class............\", type(val_data_batch))\n",
        "print(\"dataset type.............\",val_data_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B72xSyp4ZSr_",
        "outputId": "fd69127f-9b9e-4795-8fcb-d7b7398af80b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset class............ <class 'tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset'>\n",
            "dataset type............. <PaddedBatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "dataset class............ <class 'tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset'>\n",
            "dataset type............. <PaddedBatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# se entrena el modelo\n",
        "log_GRU = model_GRU.fit(\n",
        "    training_data_batch,\n",
        "    validation_data=val_data_batch,\n",
        "    callbacks=callbacks,\n",
        "    epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuInf6fc_gfa",
        "outputId": "f1300c23-4426-4fa6-9d8c-b2227105b7e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "289/289 [==============================] - 53s 174ms/step - loss: 1.5765 - accuracy: 0.3245 - val_loss: 1.5312 - val_accuracy: 0.3423\n",
            "Epoch 2/10\n",
            "289/289 [==============================] - 40s 139ms/step - loss: 1.5386 - accuracy: 0.3226 - val_loss: 1.5308 - val_accuracy: 0.3423\n",
            "Epoch 3/10\n",
            "289/289 [==============================] - 36s 124ms/step - loss: 1.5354 - accuracy: 0.3252 - val_loss: 1.5308 - val_accuracy: 0.3414\n",
            "Epoch 4/10\n",
            "289/289 [==============================] - 39s 135ms/step - loss: 1.5159 - accuracy: 0.3380 - val_loss: 1.3535 - val_accuracy: 0.4232\n",
            "Epoch 5/10\n",
            "289/289 [==============================] - 37s 128ms/step - loss: 1.2574 - accuracy: 0.4595 - val_loss: 1.2302 - val_accuracy: 0.5050\n",
            "Epoch 6/10\n",
            "289/289 [==============================] - 38s 131ms/step - loss: 1.0972 - accuracy: 0.5656 - val_loss: 1.2209 - val_accuracy: 0.5110\n",
            "Epoch 7/10\n",
            "289/289 [==============================] - 36s 126ms/step - loss: 0.9954 - accuracy: 0.6002 - val_loss: 1.2621 - val_accuracy: 0.5080\n",
            "Epoch 8/10\n",
            "289/289 [==============================] - 37s 127ms/step - loss: 0.9170 - accuracy: 0.6341 - val_loss: 1.3420 - val_accuracy: 0.4686\n",
            "Epoch 9/10\n",
            "289/289 [==============================] - 37s 128ms/step - loss: 0.8709 - accuracy: 0.6606 - val_loss: 1.4181 - val_accuracy: 0.4509\n",
            "Epoch 10/10\n",
            "289/289 [==============================] - 37s 128ms/step - loss: 0.8410 - accuracy: 0.6715 - val_loss: 1.4904 - val_accuracy: 0.4483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# se hace la gráfica las epocas del entrenamiento para ver si el modelo se sobreajusta\n",
        "plt.plot(log_GRU.history['loss'], label='Validation loss')\n",
        "plt.plot(log_GRU.history['val_loss'], label='Test loss')\n",
        "plt.title('Validation vs test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "xtjchvGKDkvr",
        "outputId": "94bfced2-7ef5-4c93-b73c-53ed2eed2ad2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### en esta gráfica se pude observar que el modelo converge alrededor de la época 5 y despues se sobreajusta (el valor de loss para train y test comienza a diverger)"
      ],
      "metadata": {
        "id": "m50HBOEgcbR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ahora se procede a hacer el summary, (antes de esta parte la función daba error)\n",
        "model_GRU.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9x9_CCD3JWp",
        "outputId": "cf395866-fe1c-4b18-c6af-ac03e077e648"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 300)              0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " Capa_Embedding (Embedding)  (None, 300, 32)           384032    \n",
            "                                                                 \n",
            " Capa_Celdas_GRU (GRU)       (None, 32)                6336      \n",
            "                                                                 \n",
            " Capa_Oculta (Dense)         (None, 32)                1056      \n",
            "                                                                 \n",
            " Capa_Salida (Dense)         (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,622\n",
            "Trainable params: 391,622\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = list()\n",
        "\n",
        "for text, labels in training_data_batch:\n",
        "    pred = model_GRU.predict(text)\n",
        "    for i in range(len(pred)):\n",
        "        y_true.append(labels[i].numpy())\n",
        "\n",
        "y_true = np.array(y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAUaqLei3T0W",
        "outputId": "7faf99db-3320-4e78-d5dd-70da9f24dc1a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 549ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 468ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(model_GRU.predict(training_data_batch), axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred=y_pred,target_names=[str(num.item() ) for num in CATEGORIES]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JONz-M5-4Xkn",
        "outputId": "bef1d097-a135-4917-d532-24abfb82890c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289/289 [==============================] - 7s 23ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.73      0.95      0.82      2024\n",
            "           2       0.43      0.61      0.51       970\n",
            "           3       0.45      0.53      0.49      1243\n",
            "           4       0.84      0.71      0.77      1974\n",
            "           5       0.65      0.45      0.53      3029\n",
            "\n",
            "    accuracy                           0.64      9240\n",
            "   macro avg       0.62      0.65      0.62      9240\n",
            "weighted avg       0.66      0.64      0.64      9240\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRITERIO 4: CONCLUSIONES DEL NEGOCIO\n",
        "\n",
        "1. dataset está no balanceado: cómo se evidenció desde el principio el dataset estaba desbalanceado, por esta razón los resultados para la clase 2 y 3 dan menor recall en comparación con la clase 5\n",
        "\n",
        "2. el mejor modelo GRU: Se realizaron algunas iteraciones con el modelo, pero el modelo más simple con el mejor desempeño fue el que se presenta, de ahí se modificaron algunos hyperparámetros para que dieran mejores resultados.\n",
        "\n",
        "Se obvio las LTSM pues este modelo es relativamene corto (300 palabras máximo) y con una arquitectura tan pequeña no debería tener desvanecimiento de gradiente. También se consultó que las [LTSM](https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b) tiene una mayor volatilidad. Por esta razón no se hicieron pruebas con el LTSM\n",
        "\n",
        "3. entendimiento del negocio, ¿cómo se modificaron los hyperparámetros?\n",
        "\n",
        "Embedding: el embedding se modificó a 300 palabras, esto con base en la longuitud máxima de los abstracts, que bajo la regulación de Harvard son 300 palabras.\n",
        "\n",
        "Vocabulario: el vocabulario se maximizó (12.000 palabras), teniendo en cuenta la especificidad del contexto, pues cada abstract tiene sus palabras específicas y maximizando esto, ayuda mucho a realizar mejores clasificaciones\n",
        "\n",
        "Entrenamiento: con un aumento progresivo de los hyperparámetros anteriormente mencionados, se observó una mejora considerable en presición y recall, por este motivo, se dejó el mejor modelo con estos parámetros maximizados.\n"
      ],
      "metadata": {
        "id": "1uGb_mkg21Lq"
      }
    }
  ]
}